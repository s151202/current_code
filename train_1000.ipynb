{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('D:\\Diamond\\code')\n",
    "from csp_james_2 import *\n",
    "\n",
    "sys.path.append('D:\\Diamond\\code')\n",
    "from thesis_funcs_19_03 import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import csv\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "\n",
    "FS= [250]\n",
    "num_epoch = 700\n",
    "\n",
    "\n",
    "\n",
    "meth = 'tl_comp_csp_mi' #gold_standï¼Œtl_comp_csp_kld , tl_comp_csp_mi\n",
    "config_root= 'E:\\\\Diamond\\\\bci_iv\\\\MODELS\\\\fbcsp_mibif_cnn\\\\2a\\\\configs\\\\'\n",
    "feature_root = 'E:\\\\Diamond\\\\bci_iv\\\\MODELS\\\\fbcsp_mibif_cnn\\\\2a\\\\CURRENT\\\\' + meth + '\\\\'\n",
    "model_root = feature_root\n",
    "save_root = model_root\n",
    "\n",
    "\n",
    "#load in cv config grid\n",
    "hp_names  =[] #all the hyper-parameter names to be validated\n",
    "with open(config_root +'cv_config.csv', mode = 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    for row in csv_reader:\n",
    "        hp_names.append((row[0]).strip())\n",
    "\n",
    "with open(config_root +'_lambda_config.csv', mode = 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    for row in csv_reader:\n",
    "        hp_names.append((row[0]).strip())\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A01T\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Diamond\\code\\thesis_funcs_19_03.py:1334: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nnF.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A02T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A03T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A04T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A05T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A06T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A07T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A08T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "A09T\n",
      "1\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "num_inits = 5\n",
    "\n",
    "for subject in range (1,10):\n",
    "\n",
    "    filename = 'A0'+str(subject)+'T'\n",
    "    filename_save = filename\n",
    "    print (filename_save)\n",
    "    \n",
    "    for portion_train in [1,0.7,0.5,0.3,0.1]:\n",
    "        print (portion_train)\n",
    "        \n",
    "        for run_win in range (0,1):\n",
    "            if run_win == 0:\n",
    "                file_root_feature =  feature_root + filename_save[:-1] + '\\\\4s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                file_root_model = model_root + filename_save[:-1] + '\\\\4s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                file_root_save = save_root + filename_save[:-1] + '\\\\4s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                #len_inp = 44\n",
    "            elif run_win == 1:\n",
    "                file_root_feature = feature_root + filename_save[:-1] + '\\\\2s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                file_root_model = model_root + filename_save[:-1] + '\\\\2s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                file_root_save = save_root + filename_save[:-1] + '\\\\2s\\\\' + 'pt_' + str(int(portion_train*100))\n",
    "                #len_inp = 25\n",
    "                \n",
    "            \n",
    "            LABELS0_go = pickle.load(open(file_root_feature + '\\\\LABELS0_go.pickle', 'rb'))\n",
    "            TRAIN_IDX = pickle.load(open(file_root_feature  + '\\\\TRAIN_IDX.pickle', 'rb'))\n",
    "            TEST_IDX = pickle.load(open(file_root_feature + '\\\\TEST_IDX.pickle', 'rb'))  \n",
    "            \n",
    "            batch_size = np.shape(TRAIN_IDX)[-1]\n",
    "            #batch_size = 50\n",
    "            \n",
    "            ###################################################################################################################\n",
    "                            #load best config\n",
    "            ###################################################################################################################\n",
    "            #load in best config line\n",
    "            config_file = open(file_root_model + '\\\\ANN\\\\best_config_val.txt', 'r')\n",
    "            config_log= config_file.readlines()\n",
    "            config_file.close()\n",
    "            for i in range (0,len(config_log)):\n",
    "                line  = config_log[(i + 1) * -1]\n",
    "                if '_act_fun_' in line: #and  line.split(' ')[0].split('_lambda_')[1] == '0':\n",
    "                    break\n",
    "\n",
    "            #extract best config values and make into dictionary\n",
    "            config = OrderedDict()\n",
    "            for hp_ind in range(0, len(hp_names)-1):\n",
    "                config[hp_names[hp_ind]] =  (line.split(hp_names[hp_ind] + '_')[1].split('_'+hp_names[hp_ind+1]+'_')[0])\n",
    "            config[hp_names[-1]] = line.split(hp_names[-1]+'_')[1].split(' ')[0]\n",
    "            \n",
    "            ACC = []\n",
    "            for n_inits in range (0, num_inits):\n",
    "                #print (n_inits)\n",
    "                ###############################################################################################################################\n",
    "                for fold in range (0, k_fold):\n",
    "                    #print (fold)\n",
    "                    train_idx0 = TRAIN_IDX[fold]\n",
    "                    test_idx0 = TEST_IDX[fold]\n",
    "\n",
    "                    trn_ind_map = np.arange(0,len(train_idx0))\n",
    "                    tst_ind_map = np.arange(0,len(test_idx0))\n",
    "\n",
    "                    random.shuffle(trn_ind_map)\n",
    "                    random.shuffle(tst_ind_map)\n",
    "\n",
    "                    y_train = LABELS0_go[train_idx0[trn_ind_map]]\n",
    "                    y_test = LABELS0_go[test_idx0[tst_ind_map]]\n",
    "                    \n",
    "                    X_train0 = pickle.load(open(file_root_feature + '\\\\Z_all_classes_train_fold_' + str(fold) + \n",
    "                                                                '_lambda_' + str(float(config['_lambda'].strip())) + \".pickle\", 'rb'))\n",
    "\n",
    "                    X_test0 = pickle.load(open( file_root_feature + '\\\\Z_all_classes_test_fold_' + str(fold) + \n",
    "                                               '_lambda_' + str(float(config['_lambda'].strip())) + \".pickle\", 'rb'))\n",
    "                    \n",
    "                    X_train0 = X_train0[trn_ind_map]\n",
    "                    X_test0 = X_test0[tst_ind_map]\n",
    "\n",
    "                    #reshape to fit 2d covolution ( 1d doesnt work)\n",
    "                    X_train = np.reshape(X_train0, [np.shape(X_train0)[0], 1, np.shape(X_train0)[1], np.shape(X_train0)[2]]).astype('float64')\n",
    "                    X_test = np.reshape(X_test0, [np.shape(X_test0)[0], 1, np.shape(X_test0)[1], np.shape(X_test0)[2]]).astype('float64')\n",
    "\n",
    "                    #convert to torch tensors for NN training\n",
    "                    X_train = torch.from_numpy(X_train).float()\n",
    "                    X_test = torch.from_numpy(X_test).float()\n",
    "                    y_train = torch.from_numpy(y_train).long()\n",
    "                    y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "                    classes_all = [0,1,2,3] \n",
    "                    \n",
    "                    #initilize model\n",
    "                    model = Model_current(chn_inp = X_train.size()[-2], len_inp = X_train.size()[-1], nf = int(config['nf']), ks = int(config['ks']) , \n",
    "                                  stride = int(config['stride']), act_f = config['act_fun'], nfc = int(config['nfc']))\n",
    "\n",
    "                    #set save_path to None if we dont want to save the nn weights\n",
    "                    save_path = None\n",
    "                    if to_save == 1:\n",
    "                        save_path = file_root_save + '\\\\ANN\\\\model_config_'+ str(make_config_str(config)) + '_' + 'n_inits_' + str(n_inits)+ '_fold_' + str(fold) + '.pt'\n",
    "                        #load in pre_trainied orignial model parameters\n",
    "                        model.load_state_dict(torch.load(save_path))\n",
    "                        \n",
    "                        #copy original (300 epochs) version of model parameters\n",
    "                        copy_path  = file_root_save + '\\\\ANN\\\\original_model_config_'+ str(make_config_str(config)) + '_' + 'n_inits_' + str(n_inits)+ '_fold_' + str(fold) + '.pt'\n",
    "                        torch.save(torch.load(save_path), copy_path)\n",
    "                        #copy original (300 epochs) version of training and testing accuracies\n",
    "                        orig_Train_acc_c = pickle.load(open(file_root_save + '\\\\ANN\\\\Train_acc_c_config_'+ make_config_str(config) + '_' + str(n_inits)+ '_fold_' + str(fold) + \".pickle\", \"rb\" ) )\n",
    "                        orig_Test_acc_c = pickle.load(open(file_root_save + '\\\\ANN\\\\Test_acc_c_config_'+ make_config_str(config) + '_' + str(n_inits)+ '_fold_' + str(fold) + \".pickle\", \"rb\" ) )\n",
    "                        pickle.dump( orig_Train_acc_c, open(file_root_save + '\\\\ANN\\\\original_Train_acc_c_config_'+ make_config_str(config) + '_' + str(n_inits)+ '_fold_' + str(fold) + \".pickle\", \"wb\" ) )\n",
    "                        pickle.dump( orig_Test_acc_c, open(file_root_save + '\\\\ANN\\\\original_Test_acc_c_config_'+ make_config_str(config) + '_' + str(n_inits)+ '_fold_' + str(fold) + \".pickle\", \"wb\" ) )\n",
    "\n",
    "                    \n",
    "                    Loss = nn.CrossEntropyLoss()\n",
    "\n",
    "                    if config['optim'] == 'SGD':\n",
    "                        lr = 0.2\n",
    "                        optimizer = optim.SGD(model.parameters(), lr = lr)\n",
    "                        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor= 0.5, min_lr=0.001, cooldown = 5)\n",
    "\n",
    "                    elif 'ADAM_' in config['optim']:\n",
    "                        lr = float(config['optim'].split('_')[1])\n",
    "                        optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "                        scheduler = None\n",
    "\n",
    "                    #Train and test NN and save the best weights\n",
    "                    Train_acc_c, Test_acc_c, Train_loss, Test_loss, best_epoch, best_eval_acc, best_eval_acc_c = train_test_NN(\n",
    "                        save_path, X_train, y_train.long(), X_test, y_test.long(), \n",
    "                                      model, Loss, optimizer, scheduler, num_epoch, batch_size, classes_all)\n",
    "                    \n",
    "                    if to_save == 1:\n",
    "                        pickle.dump( Train_acc_c, open(file_root_save + '\\\\ANN\\\\Train_acc_c_config_'+ make_config_str(config) + '_' + str(n_inits)+ '_fold_' + str(fold) + \".pickle\", \"wb\" ) )\n",
    "                        pickle.dump( Test_acc_c, open(file_root_save +  '\\\\ANN\\\\Test_acc_c_config_'+ make_config_str(config) + '_' + str(n_inits)+  '_fold_' + str(fold) + \".pickle\", \"wb\" ) )\n",
    "                    \n",
    "                    plot= 0\n",
    "                    if n_inits == 0 and plot == 1:\n",
    "                        plt.plot(np.average(Train_acc_c, axis = 1), 'b')\n",
    "                        plt.plot(np.average(Test_acc_c, axis = 1), 'r')\n",
    "                        plt.show()\n",
    "                        \n",
    "                    #calc accuracy\n",
    "                    fold_ave = np.average(best_eval_acc_c)\n",
    "                    #print ('fold', fold, ' fold acc ', fold_ave)\n",
    "                    ACC.append(fold_ave)\n",
    "                    \n",
    "            if to_save == 1:\n",
    "                orig_ACC = pickle.load(open(file_root_save + '\\\\ANN\\\\ACC_' + make_config_str(config) + '.pickle', 'rb'))\n",
    "                pickle.dump(orig_ACC, open(file_root_save + '\\\\ANN\\\\original_ACC_' + make_config_str(config) + '.pickle', 'wb'))\n",
    "                \n",
    "                pickle.dump(ACC, open(file_root_save + '\\\\ANN\\\\ACC_' + make_config_str(config) + '.pickle', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Diamond\\\\bci_iv\\\\MODELS\\\\fbcsp_mibif_cnn\\\\2a\\\\CURRENT\\\\tl_comp_csp_mi\\\\A09\\\\4s\\\\pt_10'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_root_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('act_fun', 'leaky_relu'), ('nf', '32'), ('ks', '4'), ('stride', '2'), ('nfc', '60'), ('optim', 'ADAM_0.001'), ('_lambda', '0')])\n"
     ]
    }
   ],
   "source": [
    "print (config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['act_fun', 'nf', 'ks', 'stride', 'nfc', 'optim', '_lambda']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Diamond\\\\bci_iv\\\\MODELS\\\\fbcsp_mibif_cnn\\\\2a\\\\CURRENT\\\\tl_comp_csp_mi\\\\'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
